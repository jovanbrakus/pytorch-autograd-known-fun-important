{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyTorch Autograd Engine: The Known, The Fun, and The Important\n",
    "\n",
    "While there are plenty of guides on building models and using PyTorch's autograd engine, I found myself curious about the deeper mechanics: How does automatic differentiation actually work under the hood? Where and how are the derivatives of standard functions like `sigmoid`, `matmul`, or `softmax` implemented and stored? When I call `loss.backward()`, what exactly happens in the C++ backend?"
   ],
   "id": "cde4857cb7fc5492"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# The known\n",
    "\n",
    "Autograd (automatic differentiation) is PyTorch's automatic differentiation engine that powers neural network training. It automatically computes gradients of tensor operations, which are essential for backpropagation in deep learning."
   ],
   "id": "d90b8b4f6019dbd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Core Concepts\n",
    "\n",
    "#### 1. Computational Graph\n",
    "\n",
    "PyTorch builds a dynamic computational graph (also called the autograd graph) as operations are performed. This graph consists of:\n",
    "\n",
    "- Nodes: Represent tensors (data)\n",
    "- Edges: Represent functions/operations that produced those tensors\n",
    "\n",
    "Unlike static graph frameworks (old TensorFlow v1.x, Caffe...), PyTorch's graph is built on-the-fly during the forward pass and can change between iterations.\n",
    "\n",
    "#### 2. Tensors with Gradient Tracking\n",
    "When you create a tensor with `requires_grad=True`, PyTorch starts tracking all operations on it:\n",
    "\n",
    "Each tensor has a `.grad_fn` attribute that references the function that created it\n",
    "Leaf tensors (user-created) have `grad_fn=None` but can accumulate gradients in `.grad`\n",
    "\n",
    "#### 3. The Gradient Tape Metaphor\n",
    "Think of autograd as recording operations on a \"tape\" during the forward pass. During backward pass, it replays this tape in reverse to compute gradients using the chain rule.\n"
   ],
   "id": "216925928807e9ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Key Components\n",
    "#### 1. Tensor\n",
    "\n",
    "* The fundamental data structure\n",
    "* Contains data and metadata about gradient computation\n",
    "* Has attributes like `requires_grad`, `grad`, `grad_fn`, `is_leaf`\n",
    "\n",
    "#### 2. Function (grad_fn)\n",
    "\n",
    "* Represents an operation in the graph\n",
    "* Has `forward()` and `backward()` methods\n",
    "* Stores references to input tensors and other context needed for gradient computation\n",
    "\n",
    "#### 3. Engine\n",
    "\n",
    "* Executes the backward pass\n",
    "* Manages the traversal of the computational graph\n",
    "* Handles gradient accumulation"
   ],
   "id": "7d1bbfebc63c9ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T08:18:04.281169Z",
     "start_time": "2025-09-04T08:18:03.492112Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ],
   "id": "18e2640799aeddcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using torchviz, we can easily visualize the Computational graph for smaller models. For more complex models, torchviz's diagram quickly become unreadable, in which case you may want to use [Netron](https://github.com/lutzroeder/netron) or [TensorBoard](https://www.tensorflow.org/tensorboard)\n",
   "id": "52ecc9ede3e98fb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T12:22:21.584560Z",
     "start_time": "2025-09-04T12:22:21.431329Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 13.1.2 (20250808.2320)\n -->\n<!-- Pages: 1 -->\n<svg width=\"438pt\" height=\"363pt\"\n viewBox=\"0.00 0.00 438.00 363.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 358.75)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-358.75 434,-358.75 434,4 -4,4\"/>\n<!-- 4593921600 -->\n<g id=\"node1\" class=\"node\">\n<title>4593921600</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"231,-32.75 167,-32.75 167,0 231,0 231,-32.75\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"199\" y=\"-7.25\" font-family=\"monospace\" font-size=\"10.00\"> (1, 16)</text>\n</g>\n<!-- 4590086336 -->\n<g id=\"node2\" class=\"node\">\n<title>4590086336</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"249,-89.5 149,-89.5 149,-68.75 249,-68.75 249,-89.5\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"199\" y=\"-76\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 4590086336&#45;&gt;4593921600 -->\n<g id=\"edge12\" class=\"edge\">\n<title>4590086336&#45;&gt;4593921600</title>\n<path fill=\"none\" stroke=\"black\" d=\"M199,-68.36C199,-61.89 199,-53.05 199,-44.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"202.5,-44.55 199,-34.55 195.5,-44.55 202.5,-44.55\"/>\n</g>\n<!-- 4585206640 -->\n<g id=\"node3\" class=\"node\">\n<title>4585206640</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"106,-146.25 6,-146.25 6,-125.5 106,-125.5 106,-146.25\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"56\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 4585206640&#45;&gt;4590086336 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4585206640&#45;&gt;4590086336</title>\n<path fill=\"none\" stroke=\"black\" d=\"M81.57,-125.09C104.24,-116.41 137.6,-103.63 162.98,-93.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"164.15,-97.21 172.24,-90.37 161.65,-90.68 164.15,-97.21\"/>\n</g>\n<!-- 4594022944 -->\n<g id=\"node4\" class=\"node\">\n<title>4594022944</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"88,-215.75 0,-215.75 0,-182.25 88,-182.25 88,-215.75\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"44\" y=\"-202.25\" font-family=\"monospace\" font-size=\"10.00\">Linear2.bias</text>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"44\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\"> (16)</text>\n</g>\n<!-- 4594022944&#45;&gt;4585206640 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4594022944&#45;&gt;4585206640</title>\n<path fill=\"none\" stroke=\"black\" d=\"M47.15,-181.94C48.59,-174.6 50.32,-165.82 51.86,-157.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"55.28,-158.69 53.78,-148.2 48.42,-157.34 55.28,-158.69\"/>\n</g>\n<!-- 4585206160 -->\n<g id=\"node5\" class=\"node\">\n<title>4585206160</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"249,-146.25 149,-146.25 149,-125.5 249,-125.5 249,-146.25\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"199\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 4585206160&#45;&gt;4590086336 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4585206160&#45;&gt;4590086336</title>\n<path fill=\"none\" stroke=\"black\" d=\"M199,-125.09C199,-118.47 199,-109.47 199,-101.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"202.5,-101.34 199,-91.34 195.5,-101.34 202.5,-101.34\"/>\n</g>\n<!-- 4585205008 -->\n<g id=\"node6\" class=\"node\">\n<title>4585205008</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"206,-209.38 106,-209.38 106,-188.62 206,-188.62 206,-209.38\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"156\" y=\"-195.88\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 4585205008&#45;&gt;4585206160 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4585205008&#45;&gt;4585206160</title>\n<path fill=\"none\" stroke=\"black\" d=\"M162.91,-188.18C169.03,-179.48 178.13,-166.55 185.66,-155.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"188.49,-157.9 191.38,-147.71 182.76,-153.87 188.49,-157.9\"/>\n</g>\n<!-- 4594143696 -->\n<g id=\"node7\" class=\"node\">\n<title>4594143696</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"194,-285.25 106,-285.25 106,-251.75 194,-251.75 194,-285.25\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"150\" y=\"-271.75\" font-family=\"monospace\" font-size=\"10.00\">Linear1.bias</text>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"150\" y=\"-259\" font-family=\"monospace\" font-size=\"10.00\"> (16)</text>\n</g>\n<!-- 4594143696&#45;&gt;4585205008 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4594143696&#45;&gt;4585205008</title>\n<path fill=\"none\" stroke=\"black\" d=\"M151.42,-251.5C152.23,-242.4 153.25,-230.89 154.13,-221.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"157.59,-221.6 154.99,-211.33 150.62,-220.98 157.59,-221.6\"/>\n</g>\n<!-- 4585203952 -->\n<g id=\"node8\" class=\"node\">\n<title>4585203952</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"300,-209.38 224,-209.38 224,-188.62 300,-188.62 300,-209.38\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"262\" y=\"-195.88\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 4585203952&#45;&gt;4585206160 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4585203952&#45;&gt;4585206160</title>\n<path fill=\"none\" stroke=\"black\" d=\"M251.88,-188.18C242.45,-179.04 228.2,-165.2 216.86,-154.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"219.59,-151.98 209.98,-147.52 214.72,-157 219.59,-151.98\"/>\n</g>\n<!-- 4585206832 -->\n<g id=\"node9\" class=\"node\">\n<title>4585206832</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"312,-278.88 212,-278.88 212,-258.12 312,-258.12 312,-278.88\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"262\" y=\"-265.38\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 4585206832&#45;&gt;4585203952 -->\n<g id=\"edge7\" class=\"edge\">\n<title>4585206832&#45;&gt;4585203952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M262,-257.83C262,-248.21 262,-233.13 262,-220.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"265.5,-221.17 262,-211.17 258.5,-221.17 265.5,-221.17\"/>\n</g>\n<!-- 4593910240 -->\n<g id=\"node10\" class=\"node\">\n<title>4593910240</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"312,-354.75 212,-354.75 212,-321.25 312,-321.25 312,-354.75\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"262\" y=\"-341.25\" font-family=\"monospace\" font-size=\"10.00\">Linear1.weight</text>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"262\" y=\"-328.5\" font-family=\"monospace\" font-size=\"10.00\"> (16, 8)</text>\n</g>\n<!-- 4593910240&#45;&gt;4585206832 -->\n<g id=\"edge8\" class=\"edge\">\n<title>4593910240&#45;&gt;4585206832</title>\n<path fill=\"none\" stroke=\"black\" d=\"M262,-321C262,-311.9 262,-300.39 262,-290.57\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"265.5,-290.84 262,-280.84 258.5,-290.84 265.5,-290.84\"/>\n</g>\n<!-- 4585207168 -->\n<g id=\"node11\" class=\"node\">\n<title>4585207168</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"393,-146.25 317,-146.25 317,-125.5 393,-125.5 393,-146.25\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"355\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 4585207168&#45;&gt;4590086336 -->\n<g id=\"edge9\" class=\"edge\">\n<title>4585207168&#45;&gt;4590086336</title>\n<path fill=\"none\" stroke=\"black\" d=\"M327.11,-125.09C302.12,-116.32 265.23,-103.37 237.44,-93.62\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"238.93,-90.43 228.34,-90.42 236.62,-97.04 238.93,-90.43\"/>\n</g>\n<!-- 4585206976 -->\n<g id=\"node12\" class=\"node\">\n<title>4585206976</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"424,-209.38 324,-209.38 324,-188.62 424,-188.62 424,-209.38\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"374\" y=\"-195.88\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 4585206976&#45;&gt;4585207168 -->\n<g id=\"edge10\" class=\"edge\">\n<title>4585206976&#45;&gt;4585207168</title>\n<path fill=\"none\" stroke=\"black\" d=\"M370.95,-188.18C368.36,-179.84 364.55,-167.61 361.31,-157.18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"364.74,-156.43 358.43,-147.92 358.06,-158.5 364.74,-156.43\"/>\n</g>\n<!-- 4594142656 -->\n<g id=\"node13\" class=\"node\">\n<title>4594142656</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"430,-285.25 330,-285.25 330,-251.75 430,-251.75 430,-285.25\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"380\" y=\"-271.75\" font-family=\"monospace\" font-size=\"10.00\">Linear2.weight</text>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"380\" y=\"-259\" font-family=\"monospace\" font-size=\"10.00\"> (16, 16)</text>\n</g>\n<!-- 4594142656&#45;&gt;4585206976 -->\n<g id=\"edge11\" class=\"edge\">\n<title>4594142656&#45;&gt;4585206976</title>\n<path fill=\"none\" stroke=\"black\" d=\"M378.58,-251.5C377.77,-242.4 376.75,-230.89 375.87,-221.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"379.38,-220.98 375.01,-211.33 372.41,-221.6 379.38,-220.98\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x111c5a750>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20,
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('Linear1', nn.Linear(8, 16))\n",
    "model.add_module('Linear2', nn.Linear(16, 16))\n",
    "\n",
    "x = torch.randn(1, 8)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(model.named_parameters()), show_attrs=False, show_saved=False)"
   ],
   "id": "8db7fada5020c98a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This graph shows a simple two-layer network's computation graph. Notice the `AccumulateGrad` nodes that collect gradients for each parameter (weights and biases), and the `AddmmBackward0` nodes that represent the backward pass through linear layers (Add + matrix multiplication).",
   "id": "7d576a4431570e59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Fun\n",
    "\n",
    "### ATen\n",
    "\n",
    "**ATen** (A Tensor library) is PyTorch's low-level tensor computation backend - it's the foundation that everything else builds on top of. It provides performant C++ implementations of core tensor operations that PyTorch uses (you should recognize them all):\n",
    "\n",
    "    // Basic arithmetic operations\n",
    "    at::Tensor at::add(const Tensor& self, const Tensor& other, const Scalar& alpha);\n",
    "    at::Tensor at::sub(const Tensor& self, const Tensor& other, const Scalar& alpha);\n",
    "    at::Tensor at::mul(const Tensor& self, const Tensor& other);\n",
    "    at::Tensor at::div(const Tensor& self, const Tensor& other);\n",
    "    at::Tensor at::pow(const Tensor& self, const Scalar& exponent);\n",
    "\n",
    "    // Matrix operations\n",
    "    at::Tensor at::mm(const Tensor& self, const Tensor& mat2);\n",
    "    at::Tensor at::bmm(const Tensor& self, const Tensor& mat2);\n",
    "    at::Tensor at::matmul(const Tensor& self, const Tensor& other);\n",
    "\n",
    "    // Reduction operations\n",
    "    at::Tensor at::sum(const Tensor& self, IntArrayRef dim, bool keepdim);\n",
    "    at::Tensor at::mean(const Tensor& self, IntArrayRef dim, bool keepdim);\n",
    "    at::Tensor at::max(const Tensor& self, int64_t dim, bool keepdim);\n",
    "\n",
    "    // Activation functions\n",
    "    at::Tensor at::relu(const Tensor& self);\n",
    "    at::Tensor at::sigmoid(const Tensor& self);\n",
    "    at::Tensor at::tanh(const Tensor& self);\n",
    "    at::Tensor at::softmax(const Tensor& self, int64_t dim, ScalarType dtype);\n",
    "\n",
    "All these functions are defined in `native_functions.yaml`, which can specify different C++ implementations for different platforms (CPU, CUDA, MPS, ...), which enables PyTorch's polymorphic dispatch of the function depending on the platform:\n",
    "\n",
    "    - func: add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n",
    "      device_check: NoCheck\n",
    "      device_guard: False\n",
    "      dispatch:\n",
    "        CPU: add_cpu\n",
    "        CUDA: add_cuda\n",
    "        MPS: add_mps\n",
    "      tags: pointwise\n",
    "\n",
    "\n"
   ],
   "id": "658f798109370333"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Where Do Derivatives Live?\n",
    "\n",
    "Still, these are implementations of the functions themselves, and that lib does not provide derivatives of all those functions. Those are defined in `derivatives.yaml` file.\n",
    "For simpler entries (one-liners), derivatives are defined within the yaml file itself:\n",
    "\n",
    "    - name: bmm(Tensor self, Tensor mat2) -> Tensor\n",
    "      self: grad.bmm(mat2.transpose(1, 2).conj())\n",
    "      mat2: self.transpose(1, 2).conj().bmm(grad)\n",
    "      result: self_t.bmm(mat2_p) + self_p.bmm(mat2_t)\n",
    "\n",
    "This is example of batch matrix multiplication.\n",
    "* Inputs are two 3D tensors `self` (shape: B×N×K) and `mat2` (shape: B×K×M)\n",
    "* And that gives us as output a single 3D tensor (shape: B×N×M)\n",
    "\n",
    "#### Derivation with respect to \"self\":\n",
    "\n",
    "* If output = self @ mat2, then $\\frac{\\partial \\text{output}}{\\partial \\text{self}} = \\text{mat2}^T$\n",
    "* Using chain rule: grad_self = grad_output @ $mat2^T$\n",
    "* In batch form: **grad.bmm(mat2.transpose(1, 2).conj())**\n",
    "\n",
    "#### Derivation with respect to \"mat2\":\n",
    "* If output = self @ mat2, then $\\frac{\\partial output}{\\partial mat2} = self^T$\n",
    "* Using chain rule: grad_mat2 = $self^T$ @ grad_output\n",
    "* In batch form: **self.transpose(1, 2).conj().bmm(grad)**\n",
    "\n",
    "In case when more complex implementation is needed, then the implementation of derivative function is implemented in C++ in file `FunctionsManual.cpp` (torch/csrc/autograd/FunctionsManual.cpp) and is referenced in `derivatives.yaml` (as pow_backward() is). Here is the example input for pow() function (three pow() flavors: Tensor^Scalar, Tensor^Tensor, Scalar^Tensor):\n",
    "\n",
    "    - name: pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor\n",
    "      self: pow_backward(grad, self, exponent)\n",
    "      result: auto_element_wise\n",
    "\n",
    "    - name: pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor\n",
    "      self: pow_backward_self(grad, self, exponent)\n",
    "      exponent: pow_backward_exponent(grad, self, exponent, result)\n",
    "      result: (pow_backward_self(self_t.conj(), self_p, exponent_p) + pow_backward_exponent(exponent_t.conj(), self_p, exponent_p, result)).conj()\n",
    "\n",
    "    - name: pow.Scalar(Scalar self, Tensor exponent) -> Tensor\n",
    "      exponent: pow_backward_exponent(grad, self, exponent, result)\n",
    "      result: auto_element_wise\n",
    "\n",
    "So: one-liner derivatives are defined directly in the entry in `derivatives.yaml` file, while derivatives requiring more logic/branching are implemented in `FunctionsManual.cpp` and then referenced in `derivatives.yaml`.\n"
   ],
   "id": "79de737ac99c8b7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### C++ code generation\n",
    "\n",
    "The `gen_autograd.py` script processes `native_functions.yaml` along with `derivatives.yaml` to generate the following:\n",
    "\n",
    "C++ Function Declarations - Headers for all ATen functions\n",
    "Dispatcher Registration - Maps function names to implementations\n",
    "Python Bindings - Makes functions callable from Python\n",
    "Type Checking - Validates tensor types and shapes\n",
    "Autograd Integration - Connects with gradient computation system\n",
    "\n",
    "\n",
    "To run it yourself, this is the line, with no external dependencies needed:\n",
    "\n",
    "    python tools/autograd/gen_autograd.py aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/tags.yaml build/generated tools/autograd\n",
    "\n",
    "\n",
    "You should end up with following generated files:\n",
    "\n",
    "    * Functions.h/Functions.cpp    # ATen function declarations\n",
    "    * NativeFunctions.h            # Function registry\n",
    "    * VariableType.cpp             # Autograd wrappers\n",
    "    * python_torch_functions.cpp   # Python bindings\n",
    "    * Operators.cpp                # Dispatcher registration"
   ],
   "id": "60b703894863a686"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Important\n",
    "\n",
    "If you want/need to implement new custom function you have multiple options available.\n",
    "Let's pretend that new revolutionary function you see for the first time is Swish activation function `swish = x * sigmoid(x)`.\n",
    "\n",
    "For a function as simple as this, PyTorch will handle the differentiation automatically easily, thus only forward step implementation is needed."
   ],
   "id": "c6e99636a65c1944"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T19:52:25.881939Z",
     "start_time": "2025-09-05T19:52:25.583122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, -1.0], requires_grad=True)\n",
    "y = swish(x)\n",
    "y.sum().backward()\n",
    "\n",
    "print(f\"Gradient: {x.grad}\")\n",
    "\n",
    "make_dot(y)"
   ],
   "id": "d63b46cee10569d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: tensor([0.9277, 1.0908, 0.0723])\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 13.1.2 (20250808.2320)\n -->\n<!-- Pages: 1 -->\n<svg width=\"156pt\" height=\"280pt\"\n viewBox=\"0.00 0.00 156.00 280.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 275.75)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-275.75 152,-275.75 152,4 -4,4\"/>\n<!-- 4645917296 -->\n<g id=\"node1\" class=\"node\">\n<title>4645917296</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"77,-32.75 23,-32.75 23,0 77,0 77,-32.75\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"50\" y=\"-7.25\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 4645468384 -->\n<g id=\"node2\" class=\"node\">\n<title>4645468384</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94,-89.5 6,-89.5 6,-68.75 94,-68.75 94,-89.5\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"50\" y=\"-76\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 4645468384&#45;&gt;4645917296 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4645468384&#45;&gt;4645917296</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50,-68.36C50,-61.89 50,-53.05 50,-44.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-44.55 50,-34.55 46.5,-44.55 53.5,-44.55\"/>\n</g>\n<!-- 4645454560 -->\n<g id=\"node3\" class=\"node\">\n<title>4645454560</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-203 0,-203 0,-182.25 100,-182.25 100,-203\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"50\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 4645454560&#45;&gt;4645468384 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4645454560&#45;&gt;4645468384</title>\n<path fill=\"none\" stroke=\"black\" d=\"M43.46,-182.06C37.84,-173.21 30.16,-159.48 27,-146.25 24.86,-137.28 24.86,-134.47 27,-125.5 29.15,-116.51 33.39,-107.28 37.64,-99.52\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"40.61,-101.39 42.69,-91 34.58,-97.82 40.61,-101.39\"/>\n</g>\n<!-- 4645461808 -->\n<g id=\"node5\" class=\"node\">\n<title>4645461808</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"148,-146.25 36,-146.25 36,-125.5 148,-125.5 148,-146.25\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"92\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\">SigmoidBackward0</text>\n</g>\n<!-- 4645454560&#45;&gt;4645461808 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4645454560&#45;&gt;4645461808</title>\n<path fill=\"none\" stroke=\"black\" d=\"M57.51,-181.84C63.12,-174.52 70.97,-164.29 77.76,-155.44\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"80.33,-157.84 83.64,-147.78 74.77,-153.58 80.33,-157.84\"/>\n</g>\n<!-- 4645918976 -->\n<g id=\"node4\" class=\"node\">\n<title>4645918976</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77,-271.75 23,-271.75 23,-239 77,-239 77,-271.75\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"50\" y=\"-246.25\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 4645918976&#45;&gt;4645454560 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4645918976&#45;&gt;4645454560</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50,-238.73C50,-231.35 50,-222.43 50,-214.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-214.66 50,-204.66 46.5,-214.66 53.5,-214.66\"/>\n</g>\n<!-- 4645461808&#45;&gt;4645468384 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4645461808&#45;&gt;4645468384</title>\n<path fill=\"none\" stroke=\"black\" d=\"M84.49,-125.09C78.88,-117.77 71.03,-107.54 64.24,-98.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"67.23,-96.83 58.36,-91.03 61.67,-101.09 67.23,-96.83\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x11817b430>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here you can see how PyTorch automatically constructs the graph for our swish function. The `SigmoidBackward0` and `MulBackward0` nodes show that PyTorch decomposed our operation into its components and will handle the chain rule automatically during backpropagation.",
   "id": "496005510717519a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When you need more control or specific optimisation, you would implement backward method of the function manually.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{d}{dx}[x \\cdot \\sigma(x)] &= \\frac{d}{dx}[x] \\cdot \\sigma(x) + x \\cdot \\frac{d}{dx}[\\sigma(x)] \\\\\n",
    "&= \\sigma(x) + x \\cdot \\sigma(x) \\cdot (1 - \\sigma(x)) \\\\\n",
    "&= \\sigma(x) \\cdot (1 + x \\cdot (1 - \\sigma(x)))\n",
    "\\end{align}$$\n"
   ],
   "id": "39612ca36768443d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T12:12:47.273326Z",
     "start_time": "2025-09-05T12:12:46.998356Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: tensor([0.9277, 1.0908, 0.0723])\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 13.1.2 (20250808.2320)\n -->\n<!-- Pages: 1 -->\n<svg width=\"201pt\" height=\"235pt\"\n viewBox=\"0.00 0.00 201.00 235.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 231)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-231 197,-231 197,4 -4,4\"/>\n<!-- 4699612304 -->\n<g id=\"node1\" class=\"node\">\n<title>4699612304</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"98,-32.75 44,-32.75 44,0 98,0 98,-32.75\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"71\" y=\"-7.25\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 4645784144 -->\n<g id=\"node2\" class=\"node\">\n<title>4645784144</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"142,-89.5 0,-89.5 0,-68.75 142,-68.75 142,-89.5\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"71\" y=\"-76\" font-family=\"monospace\" font-size=\"10.00\">SwishFunctionBackward</text>\n</g>\n<!-- 4645784144&#45;&gt;4699612304 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4645784144&#45;&gt;4699612304</title>\n<path fill=\"none\" stroke=\"black\" d=\"M71,-68.36C71,-61.89 71,-53.05 71,-44.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"74.5,-44.55 71,-34.55 67.5,-44.55 74.5,-44.55\"/>\n</g>\n<!-- 4645456144 -->\n<g id=\"node3\" class=\"node\">\n<title>4645456144</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"121,-152.25 21,-152.25 21,-131.5 121,-131.5 121,-152.25\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"71\" y=\"-138.75\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 4645456144&#45;&gt;4645784144 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4645456144&#45;&gt;4645784144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M71,-131.11C71,-123.01 71,-111.19 71,-100.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"74.5,-101.17 71,-91.17 67.5,-101.17 74.5,-101.17\"/>\n</g>\n<!-- 4644820368 -->\n<g id=\"node4\" class=\"node\">\n<title>4644820368</title>\n<polygon fill=\"orange\" stroke=\"black\" points=\"59,-227 5,-227 5,-194.25 59,-194.25 59,-227\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"32\" y=\"-201.5\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 4644820368&#45;&gt;4645784144 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4644820368&#45;&gt;4645784144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M22.24,-194.03C12.67,-176.59 0.99,-147.86 12,-125.5 19.88,-109.5 36.41,-97.57 49.97,-89.96\"/>\n</g>\n<!-- 4644820368&#45;&gt;4645456144 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4644820368&#45;&gt;4645456144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M41.24,-193.81C46.73,-184.41 53.74,-172.41 59.58,-162.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"62.54,-164.29 64.56,-153.89 56.49,-160.76 62.54,-164.29\"/>\n</g>\n<!-- 4646279904 -->\n<g id=\"node5\" class=\"node\">\n<title>4646279904</title>\n<polygon fill=\"orange\" stroke=\"black\" points=\"193,-158.25 139,-158.25 139,-125.5 193,-125.5 193,-158.25\"/>\n<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"166\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 4646279904&#45;&gt;4645784144 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4646279904&#45;&gt;4645784144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M141.54,-125.23C124.05,-114.05 101.08,-99.36 86.18,-89.83\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x114e81270>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31,
   "source": [
    "class SwishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Compute swish: x * sigmoid(x)\n",
    "        sigmoid = torch.sigmoid(input)\n",
    "        output = input * sigmoid\n",
    "\n",
    "        # Save tensors needed for backward\n",
    "        ctx.save_for_backward(input, sigmoid)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved tensors\n",
    "        input, sigmoid = ctx.saved_tensors\n",
    "\n",
    "        # Derivative of swish:\n",
    "        # d/dx(x * sigmoid(x)) = sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x))\n",
    "        #                      = sigmoid(x) * (1 + x * (1 - sigmoid(x)))\n",
    "\n",
    "        grad_input = grad_output * (sigmoid * (1 + input * (1 - sigmoid)))\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "# Create a convenient wrapper\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return SwishFunction.apply(x)\n",
    "\n",
    "# Test it\n",
    "x = torch.tensor([1.0, 2.0, -1.0], requires_grad=True)\n",
    "swish = Swish()\n",
    "y = swish(x)\n",
    "y.sum().backward(retain_graph=True)\n",
    "\n",
    "print(f\"Gradient: {x.grad}\")\n",
    "make_dot(y, params=dict(model.named_parameters()), show_attrs=False, show_saved=False)"
   ],
   "id": "8d659c692aecbb19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With our custom autograd function, the entire operation appears as a single `SwishFunctionBackward` node.\n",
    "This encapsulates both the forward and backward logic we defined, giving us complete control over how gradients are computed."
   ],
   "id": "8e15267f5aee7ca2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For performance reasons, you may want to implement a function in C++ with a python binding around it.\n",
    "\n",
    "    #include <torch/extension.h>\n",
    "    #include <vector>\n",
    "\n",
    "     // Forward declaration\n",
    "     std::vector<torch::Tensor> swish_forward(torch::Tensor input);\n",
    "     std::vector<torch::Tensor> swish_backward(torch::Tensor grad_output,\n",
    "                                               torch::Tensor input,\n",
    "                                               torch::Tensor sigmoid);\n",
    "\n",
    "     // Implementations\n",
    "     std::vector<torch::Tensor> swish_forward(torch::Tensor input) {\n",
    "         auto sigmoid = torch::sigmoid(input);\n",
    "         auto output = input * sigmoid;\n",
    "         return {output, sigmoid};  // Return sigmoid for backward\n",
    "     }\n",
    "\n",
    "     std::vector<torch::Tensor> swish_backward(torch::Tensor grad_output,\n",
    "                                               torch::Tensor input,\n",
    "                                               torch::Tensor sigmoid) {\n",
    "         // swish'(x) = sigmoid(x) * (1 + x * (1 - sigmoid(x)))\n",
    "         auto grad_input = grad_output * sigmoid * (1 + input * (1 - sigmoid));\n",
    "         return {grad_input};\n",
    "     }\n",
    "\n",
    "     // Python bindings\n",
    "     PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "         m.def(\"forward\", &swish_forward, \"Swish forward\");\n",
    "         m.def(\"backward\", &swish_backward, \"Swish backward\");\n",
    "     }\n",
    "\n",
    "... and python bindings:\n",
    "\n",
    "    from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "    swish_cpp = load_inline(\n",
    "        name='swish_cpp',\n",
    "        cpp_sources=[open('swish.cpp').read()],\n",
    "        functions=['forward', 'backward']\n",
    "    )\n",
    "\n",
    "    class SwishCpp(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input):\n",
    "            output, sigmoid = swish_cpp.forward(input)\n",
    "            ctx.save_for_backward(input, sigmoid)\n",
    "            return output\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            input, sigmoid = ctx.saved_tensors\n",
    "            grad_input = swish_cpp.backward(grad_output, input, sigmoid)[0]\n",
    "            return grad_input"
   ],
   "id": "a025795f8db6d161"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Further, you may want to implement multiple implementations for different backends to use PyTorch's builtin dispatcher:\n",
    "\n",
    "    import torch\n",
    "    from torch.library import Library, impl\n",
    "\n",
    "    myops = Library(\"myops\", \"DEF\")\n",
    "    myops.define(\"swish(Tensor self) -> Tensor\")\n",
    "\n",
    "    @impl(myops, \"swish\", \"CPU\")\n",
    "    def swish_cpu(x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    @impl(myops, \"swish\", \"CUDA\")\n",
    "    def swish_cuda(x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    @impl(myops, \"swish\", \"Autograd\")\n",
    "    def swish_autograd(x):\n",
    "        # Use custom autograd function\n",
    "        return SwishFunction.apply(x)\n",
    "\n",
    "    class SwishFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, x):\n",
    "            sigmoid = torch.sigmoid(x)\n",
    "            ctx.save_for_backward(x, sigmoid)\n",
    "            return x * sigmoid\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            x, sigmoid = ctx.saved_tensors\n",
    "            return grad_output * (sigmoid * (1 + x * (1 - sigmoid)))\n",
    "\n",
    "After you register it, you can access the operator directly under the Torch.ops namespace.:\n",
    "\n",
    "    x = torch.randn(5, requires_grad=True)\n",
    "    y = torch.ops.myops.swish(x)\n",
    "    y.sum().backward()\n",
    "    print(x.grad)"
   ],
   "id": "8d1a583e0b07c870"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Having issue with gradients?\n",
    "\n",
    "Ever had your model mysteriously produce NaN gradients and spent hours staring at tensors trying to figure out where things went wrong?\n",
    "The `detect_anomaly()` context manager helps debug NaN/Inf gradients by adding runtime checks during backpropagation. When enabled, PyTorch validates each gradient computation and immediately raises an exception with a detailed traceback when it encounters invalid values, pinpointing the exact operation that failed. While this slows down training considerably, it transforms opaque NaN errors into actionable debugging information, making it invaluable for diagnosing gradient instability issues during development.\n",
    "\n",
    "    with torch.autograd.detect_anomaly():\n",
    "        y = model(x)\n",
    "        loss = criterion(y, target)\n",
    "        loss.backward()  # Will provide detailed error info if NaN/Inf"
   ],
   "id": "bce7076db0aef8a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Gradient checkpointing\n",
    "\n",
    "If working on a local machine, gradient checkpointing can be very useful as it's a memory-saving technique that trades computation time for memory usage. Here's how it works:\n",
    "\n",
    "#### Normal Forward Pass (without checkpointing)\n",
    "\n",
    "    x = self.layer1(x)  # Computes output AND stores all intermediate activations\n",
    "    x = self.layer2(x)  # Uses layer1's output, stores its own activations too\n",
    "    loss = criterion(x)\n",
    "    loss.backward()     # Uses stored activations to compute gradients\n",
    "\n",
    "During normal forward pass, PyTorch stores all intermediate tensors needed for the backward pass. For `layer1`, this includes:\n",
    "\n",
    "* Input activations\n",
    "* Weight matrices\n",
    "* Any intermediate computations (like pre-activation values)\n",
    "For a large model, this can consume massive amounts of memory.\n",
    "\n",
    "#### With Checkpointing\n",
    "    x = checkpoint(self.layer1, x)  # Computes output but DISCARDS intermediate activations\n",
    "    x = self.layer2(x)               # Normal forward pass\n",
    "    loss = criterion(x)\n",
    "    loss.backward()                  # Re-computes layer1 forward pass when needed\n",
    "\n",
    "Here's what happens in this case:\n",
    "\n",
    "1) Forward pass: `checkpoint(self.layer1, x)` runs layer1 forward but only keeps the final output, **discarding intermediate activations to save memory**.\n",
    "2) Backward pass: When gradients reach `layer1`, PyTorch:\n",
    "    * Re-runs the forward pass of `layer1` to recreate the intermediate activations\n",
    "    * Then computes gradients normally using these recreated activations\n",
    "    * This **doubles the computation time** for `layer1` but saves memory"
   ],
   "id": "a5b408c75215b339"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Understanding PyTorch's autograd internals lets you:\n",
    "\n",
    "* Debug gradient flow issues when your model won't train\n",
    "* Implement custom operations that autograd can't handle automatically\n",
    "* Optimize memory and computation for large-scale training\n",
    "* Extend PyTorch with domain-specific functions that are actually fast\n",
    "\n",
    "The key takeaways:\n",
    "\n",
    "1) **The Known**: PyTorch builds dynamic graphs that can change every iteration\n",
    "2) **The Fun**: Derivatives live in YAML files and get compiled to C++\n",
    "3) **The Important**: You can extend autograd at multiple levels, from pure Python to CUDA kernels\n",
    "\n",
    "Whether you're debugging why your gradients exploded or implementing that paper's weird activation function, knowing how autograd works under the hood turns mysterious errors into solvable problems."
   ],
   "id": "5354d787819a01bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
